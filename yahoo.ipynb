{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_expert_annotations.tsv\",\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_turk_annotations.tsv\",\n",
    "]\n",
    "\n",
    "datasets = []\n",
    "for file in files:\n",
    "    datasets.append(pd.read_csv(file, sep=\"\\t\"))\n",
    "\n",
    "yahoo_df = pd.concat(datasets, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yahoo_df.shape)\n",
    "yahoo_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the comment threads at each comment level in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import contractions\n",
    "from num2words import num2words\n",
    "\n",
    "\n",
    "def numbers_to_words(text: str) -> str:\n",
    "    t = text.split()\n",
    "    for ind, word in enumerate(t):\n",
    "        if all(c.isdigit() for c in word):\n",
    "            t[ind] = num2words(word)\n",
    "        elif (\n",
    "            len(word) > 2\n",
    "            and all(c.isdigit() for c in word[:-2])\n",
    "            and word[-2:] in [\"st\", \"nd\", \"rd\", \"th\"]\n",
    "        ):\n",
    "            t[ind] = num2words(int(word[:-2]), to=\"ordinal\")\n",
    "\n",
    "    return \" \".join(t)\n",
    "\n",
    "\n",
    "def build_comment_thread(row: pd.Series) -> str:\n",
    "    if not row[\"text\"]:\n",
    "        return \"\"\n",
    "    if row[\"text\"][-1] not in string.punctuation:\n",
    "        row[\"text\"] += \".\"\n",
    "\n",
    "    if row[\"commentindex\"] != 0:\n",
    "        parent_df = yahoo_df[yahoo_df.commentid == row[\"parentid\"]]\n",
    "        if parent_df.shape[0] == 0:\n",
    "            return f\"{row['headline']}. {row['text']}\"\n",
    "        else:\n",
    "            return f\"{parent_df.iloc[0].thread}. {row['text']}\"\n",
    "    else:\n",
    "        return row[\"text\"]\n",
    "\n",
    "\n",
    "yahoo_df[\"text\"] = (\n",
    "    yahoo_df[\"text\"]\n",
    "    .str.replace(r\"[^\\w\\s]+\", \"\", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.replace(\"..\", \".\", regex=False)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .apply(lambda x: contractions.fix(x, slang=False))\n",
    "    .apply(numbers_to_words)\n",
    "    .astype(str)\n",
    ")\n",
    "yahoo_df = yahoo_df.sort_values(by=[\"commentindex\"])\n",
    "yahoo_df[\"thread\"] = \"\"\n",
    "for index, row in yahoo_df.iterrows():\n",
    "    yahoo_df.at[index, \"thread\"] = build_comment_thread(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_train-ids.txt\"\n",
    ") as f:\n",
    "    train_ids = [int(x) for x in f.read().splitlines()]\n",
    "\n",
    "with open(\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_dev-ids.txt\"\n",
    ") as f:\n",
    "    dev_ids = [int(x) for x in f.read().splitlines()]\n",
    "\n",
    "with open(\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_test-ids.txt\"\n",
    ") as f:\n",
    "    test_ids = [int(x) for x in f.read().splitlines()]\n",
    "\n",
    "train_df = yahoo_df[yahoo_df[\"sdid\"].isin(train_ids)].copy(deep=True)\n",
    "dev_df = yahoo_df[yahoo_df[\"sdid\"].isin(dev_ids)].copy(deep=True)\n",
    "test_df = yahoo_df[yahoo_df[\"sdid\"].isin(test_ids)].copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    (\"persuasiveness\", \"Persuasiveness\"),\n",
    "    (\"constructiveclass\", \"Constructiveness\"),\n",
    "    (\"intendedaudience\", \"Intended Audience\"),\n",
    "]\n",
    "for column_name, display_name in cols:\n",
    "    data = (\n",
    "        train_df.fillna(\"Unlabeled\")\n",
    "        .groupby([\"commentindex\", column_name])[\"sdid\"]\n",
    "        .count()\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"commentindex\"] += 1\n",
    "    data = pd.pivot_table(\n",
    "        data, values=\"sdid\", index=[\"commentindex\"], columns=[column_name]\n",
    "    )\n",
    "    data = data.sort_values(by=\"commentindex\", ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    data.plot(kind=\"barh\", stacked=True, ax=ax)\n",
    "    ax.set(ylabel=\"Comment Indices\", xlabel=\"Count\")\n",
    "    ax.set_title(f\"Comment Index Frequencies by {display_name.capitalize()}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tone\", \"topic\"]\n",
    "for column_name in cols:\n",
    "    unique_values = list(yahoo_df[column_name].str.split(\",\").explode().unique())\n",
    "    unique_values.remove(np.NaN)\n",
    "    unique_values.remove(\"NA\")\n",
    "\n",
    "    for value in unique_values:\n",
    "        data = yahoo_df.fillna(\"Unlabeled\")\n",
    "        data[value] = yahoo_df[column_name].str.contains(value)\n",
    "\n",
    "        data = (\n",
    "            data.fillna(\"Unlabeled\")\n",
    "            .groupby([\"commentindex\", value])[\"sdid\"]\n",
    "            .count()\n",
    "            .reset_index()\n",
    "        )\n",
    "        data[\"commentindex\"] += 1\n",
    "        data = pd.pivot_table(\n",
    "            data, values=\"sdid\", index=[\"commentindex\"], columns=[value]\n",
    "        )\n",
    "        data = data.sort_values(by=\"commentindex\", ascending=False)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        data.plot(kind=\"barh\", stacked=True, ax=ax)\n",
    "        ax.set(ylabel=\"Comment Indices\", xlabel=\"Count\")\n",
    "        ax.set_title(f\"Comment Index Frequencies by {value.capitalize()}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, dev_df, test_df]:\n",
    "    df[\"controversial\"] = (\n",
    "        df[\"tone\"].str.contains(\"controversial\", case=False).fillna(False).astype(str)\n",
    "    )\n",
    "    df[\"mean\"] = df[\"tone\"].str.contains(\"mean\", case=False).fillna(False).astype(str)\n",
    "    df[\"sarcastic\"] = (\n",
    "        df[\"tone\"].str.contains(\"sarcastic\", case=False).fillna(False).astype(str)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.DataFrame(columns=[\"Dataset\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "for target_col, display_name in [\n",
    "    (\"persuasiveness\", \"Persuasiveness\"),\n",
    "    (\"constructiveclass\", \"Constructiveness\"),\n",
    "    (\"intendedaudience\", \"Intended Audience\"),\n",
    "    (\"controversial\", \"Controversial\"),\n",
    "    (\"mean\", \"Mean\"),\n",
    "    (\"sarcastic\", \"Sarcastic\"),\n",
    "]:\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_X = vectorizer.fit_transform(train_df[\"text\"])\n",
    "    dev_X = vectorizer.transform(dev_df[\"text\"])\n",
    "    test_X = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "    train_y = train_df[target_col]\n",
    "    dev_y = dev_df[target_col]\n",
    "    test_y = test_df[target_col]\n",
    "\n",
    "    train_X = train_X[train_y.notna()]\n",
    "    train_y = train_y[train_y.notna()]\n",
    "\n",
    "    dev_X = dev_X[dev_y.notna()]\n",
    "    dev_y = dev_y[dev_y.notna()]\n",
    "\n",
    "    test_X = test_X[test_y.notna()]\n",
    "    test_y = test_y[test_y.notna()]\n",
    "\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    clf.fit(train_X, train_y)\n",
    "\n",
    "    pred_dev_y = clf.predict(dev_X)\n",
    "    pred_test_y = clf.predict(test_X)\n",
    "\n",
    "    baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "        display_name,\n",
    "        f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "        f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.DataFrame(columns=[\"Dataset\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "for target_col, display_name in [\n",
    "    (\"persuasiveness\", \"Persuasiveness\"),\n",
    "    (\"constructiveclass\", \"Constructiveness\"),\n",
    "    (\"intendedaudience\", \"Intended Audience\"),\n",
    "    (\"controversial\", \"Controversial\"),\n",
    "    (\"mean\", \"Mean\"),\n",
    "    (\"sarcastic\", \"Sarcastic\"),\n",
    "]:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_X = vectorizer.fit_transform(train_df[\"text\"])\n",
    "    dev_X = vectorizer.transform(dev_df[\"text\"])\n",
    "    test_X = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "    train_y = train_df[target_col]\n",
    "    dev_y = dev_df[target_col]\n",
    "    test_y = test_df[target_col]\n",
    "\n",
    "    train_X = train_X[train_y.notna()]\n",
    "    train_y = train_y[train_y.notna()]\n",
    "\n",
    "    dev_X = dev_X[dev_y.notna()]\n",
    "    dev_y = dev_y[dev_y.notna()]\n",
    "\n",
    "    test_X = test_X[test_y.notna()]\n",
    "    test_y = test_y[test_y.notna()]\n",
    "\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    clf.fit(train_X, train_y)\n",
    "\n",
    "    pred_dev_y = clf.predict(dev_X)\n",
    "    pred_test_y = clf.predict(test_X)\n",
    "\n",
    "    baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "        display_name,\n",
    "        f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "        f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "\n",
    "def get_average_w2v_vector(row):\n",
    "    words = row[\"text\"].split()\n",
    "\n",
    "    avg_text_w2v = np.sum([w2v_model[w] for w in words if w in w2v_model], axis=0) / (\n",
    "        len(words) if words else 1\n",
    "    )\n",
    "\n",
    "    if avg_text_w2v.shape != (300,):\n",
    "        avg_text_w2v = np.zeros((300,))\n",
    "\n",
    "    return avg_text_w2v\n",
    "\n",
    "\n",
    "baseline_scores = pd.DataFrame(columns=[\"Dataset\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "for target_col, display_name in [\n",
    "    (\"persuasiveness\", \"Persuasiveness\"),\n",
    "    (\"constructiveclass\", \"Constructiveness\"),\n",
    "    (\"intendedaudience\", \"Intended Audience\"),\n",
    "    (\"controversial\", \"Controversial\"),\n",
    "    (\"mean\", \"Mean\"),\n",
    "    (\"sarcastic\", \"Sarcastic\"),\n",
    "]:\n",
    "    train_X = train_df.apply(\n",
    "        lambda x: get_average_w2v_vector(x), axis=1, result_type=\"expand\"\n",
    "    )\n",
    "    dev_X = dev_df.apply(\n",
    "        lambda x: get_average_w2v_vector(x), axis=1, result_type=\"expand\"\n",
    "    )\n",
    "    test_X = test_df.apply(\n",
    "        lambda x: get_average_w2v_vector(x), axis=1, result_type=\"expand\"\n",
    "    )\n",
    "\n",
    "    train_y = train_df[target_col]\n",
    "    dev_y = dev_df[target_col]\n",
    "    test_y = test_df[target_col]\n",
    "\n",
    "    train_X = train_X[train_y.notna()]\n",
    "    train_y = train_y[train_y.notna()]\n",
    "\n",
    "    dev_X = dev_X[dev_y.notna()]\n",
    "    dev_y = dev_y[dev_y.notna()]\n",
    "\n",
    "    test_X = test_X[test_y.notna()]\n",
    "    test_y = test_y[test_y.notna()]\n",
    "\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    clf.fit(train_X, train_y)\n",
    "\n",
    "    pred_dev_y = clf.predict(dev_X)\n",
    "    pred_test_y = clf.predict(test_X)\n",
    "\n",
    "    baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "        display_name,\n",
    "        f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "        f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model with Extractive Summary Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.language = \"english\"\n",
    "        self.num_sentences = 3\n",
    "        self.tokenizer = Tokenizer(self.language)\n",
    "        self.stemmer = Stemmer(self.language)\n",
    "\n",
    "    def summarize(self, thread: str) -> str:\n",
    "        parser = PlaintextParser.from_string(thread, self.tokenizer)\n",
    "        return \"\".join(\n",
    "            (x._text for x in self.summarizer(parser.document, self.num_sentences))\n",
    "        )\n",
    "\n",
    "\n",
    "class Luhn(Summarizer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.summarizer = LuhnSummarizer()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Luhn\"\n",
    "\n",
    "\n",
    "class LSA(Summarizer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.summarizer = LsaSummarizer()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"LSA\"\n",
    "\n",
    "\n",
    "class LexRank(Summarizer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.summarizer = LexRankSummarizer()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"LexRank\"\n",
    "\n",
    "\n",
    "class TextRank(Summarizer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.summarizer = TextRankSummarizer()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"TextRank\"\n",
    "\n",
    "\n",
    "summarizer_models = [Luhn(), LSA(), LexRank(), TextRank()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.DataFrame(columns=[\"Summarizer\", \"Target\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "\n",
    "for summarizer in summarizer_models:\n",
    "    train_df[\"summary\"] = train_df[\"thread\"].apply(summarizer.summarize)\n",
    "    dev_df[\"summary\"] = dev_df[\"thread\"].apply(summarizer.summarize)\n",
    "    test_df[\"summary\"] = test_df[\"thread\"].apply(summarizer.summarize)\n",
    "\n",
    "    for target_col, display_name in [\n",
    "        (\"persuasiveness\", \"Persuasiveness\"),\n",
    "        (\"constructiveclass\", \"Constructiveness\"),\n",
    "        (\"intendedaudience\", \"Intended Audience\"),\n",
    "        (\"controversial\", \"Controversial\"),\n",
    "        (\"mean\", \"Mean\"),\n",
    "        (\"sarcastic\", \"Sarcastic\"),\n",
    "    ]:\n",
    "\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        train_X = vectorizer.fit_transform(train_df[\"summary\"])\n",
    "        dev_X = vectorizer.transform(dev_df[\"summary\"])\n",
    "        test_X = vectorizer.transform(test_df[\"summary\"])\n",
    "\n",
    "        train_y = train_df[target_col]\n",
    "        dev_y = dev_df[target_col]\n",
    "        test_y = test_df[target_col]\n",
    "\n",
    "        train_X = train_X[train_y.notna()]\n",
    "        train_y = train_y[train_y.notna()]\n",
    "\n",
    "        dev_X = dev_X[dev_y.notna()]\n",
    "        dev_y = dev_y[dev_y.notna()]\n",
    "\n",
    "        test_X = test_X[test_y.notna()]\n",
    "        test_y = test_y[test_y.notna()]\n",
    "\n",
    "        clf = LogisticRegression(max_iter=10000)\n",
    "        clf.fit(train_X, train_y)\n",
    "\n",
    "        pred_dev_y = clf.predict(dev_X)\n",
    "        pred_test_y = clf.predict(test_X)\n",
    "\n",
    "        baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "            str(summarizer),\n",
    "            display_name,\n",
    "            f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "            f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "        ]\n",
    "\n",
    "    train_df = train_df.drop([\"summary\"], axis=1)\n",
    "    dev_df = dev_df.drop([\"summary\"], axis=1)\n",
    "    test_df = test_df.drop([\"summary\"], axis=1)\n",
    "\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.DataFrame(columns=[\"Summarizer\", \"Target\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "\n",
    "for summarizer in summarizer_models:\n",
    "    train_df[\"summary\"] = train_df[\"thread\"].apply(summarizer.summarize)\n",
    "    dev_df[\"summary\"] = dev_df[\"thread\"].apply(summarizer.summarize)\n",
    "    test_df[\"summary\"] = test_df[\"thread\"].apply(summarizer.summarize)\n",
    "\n",
    "    for target_col, display_name in [\n",
    "        (\"persuasiveness\", \"Persuasiveness\"),\n",
    "        (\"constructiveclass\", \"Constructiveness\"),\n",
    "        (\"intendedaudience\", \"Intended Audience\"),\n",
    "        (\"controversial\", \"Controversial\"),\n",
    "        (\"mean\", \"Mean\"),\n",
    "        (\"sarcastic\", \"Sarcastic\"),\n",
    "    ]:\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        train_X = vectorizer.fit_transform(train_df[\"summary\"])\n",
    "        dev_X = vectorizer.transform(dev_df[\"summary\"])\n",
    "        test_X = vectorizer.transform(test_df[\"summary\"])\n",
    "\n",
    "        train_y = train_df[target_col]\n",
    "        dev_y = dev_df[target_col]\n",
    "        test_y = test_df[target_col]\n",
    "\n",
    "        train_X = train_X[train_y.notna()]\n",
    "        train_y = train_y[train_y.notna()]\n",
    "\n",
    "        dev_X = dev_X[dev_y.notna()]\n",
    "        dev_y = dev_y[dev_y.notna()]\n",
    "\n",
    "        test_X = test_X[test_y.notna()]\n",
    "        test_y = test_y[test_y.notna()]\n",
    "\n",
    "        clf = LogisticRegression(max_iter=10000)\n",
    "        clf.fit(train_X, train_y)\n",
    "\n",
    "        pred_dev_y = clf.predict(dev_X)\n",
    "        pred_test_y = clf.predict(test_X)\n",
    "\n",
    "        baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "            str(summarizer),\n",
    "            display_name,\n",
    "            f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "            f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "        ]\n",
    "\n",
    "    train_df = train_df.drop([\"summary\"], axis=1)\n",
    "    dev_df = dev_df.drop([\"summary\"], axis=1)\n",
    "    test_df = test_df.drop([\"summary\"], axis=1)\n",
    "\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "\n",
    "def get_average_w2v_vector_thread(row):\n",
    "    words = row[\"text\"].split()\n",
    "    summary_words = row[\"summary\"].split()\n",
    "\n",
    "    avg_text_w2v = np.sum([w2v_model[w] for w in words if w in w2v_model], axis=0) / (\n",
    "        len(words) if words else 1\n",
    "    )\n",
    "\n",
    "    if avg_text_w2v.shape != (300,):\n",
    "        avg_text_w2v = np.zeros((300,))\n",
    "\n",
    "    avg_thread_w2v = np.sum(\n",
    "        [w2v_model[w] for w in summary_words if w in w2v_model], axis=0\n",
    "    ) / (len(summary_words) if summary_words else 1)\n",
    "\n",
    "    if avg_thread_w2v.shape != (300,):\n",
    "        avg_thread_w2v = np.zeros((300,))\n",
    "\n",
    "    embedding = np.concatenate((avg_text_w2v, avg_thread_w2v), axis=None)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "baseline_scores = pd.DataFrame(columns=[\"Summarizer\", \"Target\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "\n",
    "for summarizer in summarizer_models:\n",
    "    train_df[\"summary\"] = train_df[\"thread\"].apply(summarizer.summarize)\n",
    "    dev_df[\"summary\"] = dev_df[\"thread\"].apply(summarizer.summarize)\n",
    "    test_df[\"summary\"] = test_df[\"thread\"].apply(summarizer.summarize)\n",
    "\n",
    "    for target_col, display_name in [\n",
    "        (\"persuasiveness\", \"Persuasiveness\"),\n",
    "        (\"constructiveclass\", \"Constructiveness\"),\n",
    "        (\"intendedaudience\", \"Intended Audience\"),\n",
    "        (\"controversial\", \"Controversial\"),\n",
    "        (\"mean\", \"Mean\"),\n",
    "        (\"sarcastic\", \"Sarcastic\"),\n",
    "    ]:\n",
    "        train_X = train_df.apply(\n",
    "            lambda x: get_average_w2v_vector_thread(x), axis=1, result_type=\"expand\"\n",
    "        )\n",
    "        dev_X = dev_df.apply(\n",
    "            lambda x: get_average_w2v_vector_thread(x), axis=1, result_type=\"expand\"\n",
    "        )\n",
    "        test_X = test_df.apply(\n",
    "            lambda x: get_average_w2v_vector_thread(x), axis=1, result_type=\"expand\"\n",
    "        )\n",
    "\n",
    "        train_y = train_df[target_col]\n",
    "        dev_y = dev_df[target_col]\n",
    "        test_y = test_df[target_col]\n",
    "\n",
    "        train_X = train_X[train_y.notna()]\n",
    "        train_y = train_y[train_y.notna()]\n",
    "\n",
    "        dev_X = dev_X[dev_y.notna()]\n",
    "        dev_y = dev_y[dev_y.notna()]\n",
    "\n",
    "        test_X = test_X[test_y.notna()]\n",
    "        test_y = test_y[test_y.notna()]\n",
    "\n",
    "        clf = LogisticRegression(max_iter=10000)\n",
    "        clf.fit(train_X, train_y)\n",
    "\n",
    "        pred_dev_y = clf.predict(dev_X)\n",
    "        pred_test_y = clf.predict(test_X)\n",
    "\n",
    "        baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "            str(summarizer),\n",
    "            display_name,\n",
    "            f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "            f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "        ]\n",
    "\n",
    "    train_df = train_df.drop([\"summary\"], axis=1)\n",
    "    dev_df = dev_df.drop([\"summary\"], axis=1)\n",
    "    test_df = test_df.drop([\"summary\"], axis=1)\n",
    "\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show essentially no changes aside from a small improvement for constructiveness. Constructiveness does have the most well-defined labels but maybe we need more complex features or models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with sentence embedding features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer(\"stsb-distilroberta-base-v2\")\n",
    "\n",
    "train_sentence_embeddings = {}\n",
    "dev_entence_embeddings = {}\n",
    "test_sentence_embeddings = {}\n",
    "\n",
    "train_sentence_embeddings[\"Base\"] = sbert_model.encode(train_df[\"text\"].to_list())\n",
    "dev_entence_embeddings[\"Base\"] = sbert_model.encode(dev_df[\"text\"].to_list())\n",
    "test_sentence_embeddings[\"Base\"] = sbert_model.encode(test_df[\"text\"].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for summarizer in summarizer_models:\n",
    "    train_sentence_embeddings[str(summarizer)] = sbert_model.encode(\n",
    "        train_df[\"thread\"].apply(summarizer.summarize).to_list()\n",
    "    )\n",
    "    dev_entence_embeddings[str(summarizer)] = sbert_model.encode(\n",
    "        dev_df[\"thread\"].apply(summarizer.summarize).to_list()\n",
    "    )\n",
    "    test_sentence_embeddings[str(summarizer)] = sbert_model.encode(\n",
    "        test_df[\"thread\"].apply(summarizer.summarize).to_list()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.DataFrame(columns=[\"Dataset\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "\n",
    "for target_col, display_name in [\n",
    "    (\"persuasiveness\", \"Persuasiveness\"),\n",
    "    (\"constructiveclass\", \"Constructiveness\"),\n",
    "    (\"intendedaudience\", \"Intended Audience\"),\n",
    "    (\"controversial\", \"Controversial\"),\n",
    "    (\"mean\", \"Mean\"),\n",
    "    (\"sarcastic\", \"Sarcastic\"),\n",
    "]:\n",
    "    train_X = train_sentence_embeddings[\"Base\"].copy()\n",
    "    dev_X = dev_entence_embeddings[\"Base\"].copy()\n",
    "    test_X = test_sentence_embeddings[\"Base\"].copy()\n",
    "\n",
    "    train_y = train_df[target_col]\n",
    "    dev_y = dev_df[target_col]\n",
    "    test_y = test_df[target_col]\n",
    "\n",
    "    train_X = train_X[train_y.notna()]\n",
    "    train_y = train_y[train_y.notna()]\n",
    "\n",
    "    dev_X = dev_X[dev_y.notna()]\n",
    "    dev_y = dev_y[dev_y.notna()]\n",
    "\n",
    "    test_X = test_X[test_y.notna()]\n",
    "    test_y = test_y[test_y.notna()]\n",
    "\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    clf.fit(train_X, train_y)\n",
    "\n",
    "    pred_dev_y = clf.predict(dev_X)\n",
    "    pred_test_y = clf.predict(test_X)\n",
    "\n",
    "    baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "        display_name,\n",
    "        f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "        f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.DataFrame(columns=[\"Summarizer\", \"Target\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "for summarizer in summarizer_models:\n",
    "    for target_col, display_name in [\n",
    "        (\"persuasiveness\", \"Persuasiveness\"),\n",
    "        (\"constructiveclass\", \"Constructiveness\"),\n",
    "        (\"intendedaudience\", \"Intended Audience\"),\n",
    "        (\"controversial\", \"Controversial\"),\n",
    "        (\"mean\", \"Mean\"),\n",
    "        (\"sarcastic\", \"Sarcastic\"),\n",
    "    ]:\n",
    "        train_X = np.concatenate(\n",
    "            (\n",
    "                train_sentence_embeddings[\"Base\"],\n",
    "                train_sentence_embeddings[str(summarizer)],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        dev_X = np.concatenate(\n",
    "            (dev_entence_embeddings[\"Base\"], dev_entence_embeddings[str(summarizer)]),\n",
    "            axis=1,\n",
    "        )\n",
    "        test_X = np.concatenate(\n",
    "            (\n",
    "                test_sentence_embeddings[\"Base\"],\n",
    "                test_sentence_embeddings[str(summarizer)],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        train_y = train_df[target_col]\n",
    "        dev_y = dev_df[target_col]\n",
    "        test_y = test_df[target_col]\n",
    "\n",
    "        train_X = train_X[train_y.notna()]\n",
    "        train_y = train_y[train_y.notna()]\n",
    "\n",
    "        dev_X = dev_X[dev_y.notna()]\n",
    "        dev_y = dev_y[dev_y.notna()]\n",
    "\n",
    "        test_X = test_X[test_y.notna()]\n",
    "        test_y = test_y[test_y.notna()]\n",
    "\n",
    "        clf = LogisticRegression(max_iter=10000)\n",
    "        clf.fit(train_X, train_y)\n",
    "\n",
    "        pred_dev_y = clf.predict(dev_X)\n",
    "        pred_test_y = clf.predict(test_X)\n",
    "\n",
    "        baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "            str(summarizer),\n",
    "            display_name,\n",
    "            f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "            f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "        ]\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out a better classification models with sentence embeddings features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.DataFrame(columns=[\"Dataset\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "enc = LabelEncoder()\n",
    "\n",
    "for target_col, display_name in [\n",
    "    (\"persuasiveness\", \"Persuasiveness\"),\n",
    "    (\"constructiveclass\", \"Constructiveness\"),\n",
    "    (\"intendedaudience\", \"Intended Audience\"),\n",
    "    (\"controversial\", \"Controversial\"),\n",
    "    (\"mean\", \"Mean\"),\n",
    "    (\"sarcastic\", \"Sarcastic\"),\n",
    "]:\n",
    "    train_X = train_sentence_embeddings[\"Base\"].copy()\n",
    "    dev_X = dev_entence_embeddings[\"Base\"].copy()\n",
    "    test_X = test_sentence_embeddings[\"Base\"].copy()\n",
    "\n",
    "    train_y = train_df[target_col].copy()\n",
    "    dev_y = dev_df[target_col].copy()\n",
    "    test_y = test_df[target_col].copy()\n",
    "\n",
    "    train_X = train_X[train_y.notna()]\n",
    "    train_y = enc.fit_transform(train_y[train_y.notna()])\n",
    "\n",
    "    dev_X = dev_X[dev_y.notna()]\n",
    "    dev_y = enc.transform(dev_y[dev_y.notna()])\n",
    "\n",
    "    test_X = test_X[test_y.notna()]\n",
    "    test_y = enc.transform(test_y[test_y.notna()])\n",
    "\n",
    "    clf = XGBClassifier()\n",
    "    clf.fit(train_X, train_y)\n",
    "\n",
    "    pred_dev_y = clf.predict(dev_X)\n",
    "    pred_test_y = clf.predict(test_X)\n",
    "\n",
    "    baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "        display_name,\n",
    "        f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "        f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.DataFrame(columns=[\"Summarizer\", \"Target\", \"Dev F1\", \"Test F1\"])\n",
    "\n",
    "for summarizer in summarizer_models:\n",
    "    for target_col, display_name in [\n",
    "        (\"persuasiveness\", \"Persuasiveness\"),\n",
    "        (\"constructiveclass\", \"Constructiveness\"),\n",
    "        (\"intendedaudience\", \"Intended Audience\"),\n",
    "        (\"controversial\", \"Controversial\"),\n",
    "        (\"mean\", \"Mean\"),\n",
    "        (\"sarcastic\", \"Sarcastic\"),\n",
    "    ]:\n",
    "        train_X = np.concatenate(\n",
    "            (\n",
    "                train_sentence_embeddings[\"Base\"],\n",
    "                train_sentence_embeddings[str(summarizer)],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        dev_X = np.concatenate(\n",
    "            (dev_entence_embeddings[\"Base\"], dev_entence_embeddings[str(summarizer)]),\n",
    "            axis=1,\n",
    "        )\n",
    "        test_X = np.concatenate(\n",
    "            (\n",
    "                test_sentence_embeddings[\"Base\"],\n",
    "                test_sentence_embeddings[str(summarizer)],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        train_y = train_df[target_col]\n",
    "        dev_y = dev_df[target_col]\n",
    "        test_y = test_df[target_col]\n",
    "\n",
    "        train_X = train_X[train_y.notna()]\n",
    "        train_y = enc.fit_transform(train_y[train_y.notna()])\n",
    "\n",
    "        dev_X = dev_X[dev_y.notna()]\n",
    "        dev_y = enc.transform(dev_y[dev_y.notna()])\n",
    "\n",
    "        test_X = test_X[test_y.notna()]\n",
    "        test_y = enc.transform(test_y[test_y.notna()])\n",
    "\n",
    "        clf = XGBClassifier()\n",
    "        clf.fit(train_X, train_y)\n",
    "\n",
    "        pred_dev_y = clf.predict(dev_X)\n",
    "        pred_test_y = clf.predict(test_X)\n",
    "\n",
    "        baseline_scores.loc[len(baseline_scores.index)] = [\n",
    "            str(summarizer),\n",
    "            display_name,\n",
    "            f1_score(dev_y, pred_dev_y, average=\"weighted\"),\n",
    "            f1_score(test_y, pred_test_y, average=\"weighted\"),\n",
    "        ]\n",
    "\n",
    "baseline_scores.round(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "085f93f36de165c847c7a3bd2a9829506180f377b36f2a484d22d83cff3213f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
