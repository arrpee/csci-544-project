{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import contractions\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from num2words import num2words\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_df = pd.read_csv(\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_expert_annotations.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbers_to_words(text: str) -> str:\n",
    "    t = text.split()\n",
    "    for ind, word in enumerate(t):\n",
    "        if all(c.isdigit() for c in word):\n",
    "            t[ind] = num2words(word)\n",
    "        elif (\n",
    "            len(word) > 2\n",
    "            and all(c.isdigit() for c in word[:-2])\n",
    "            and word[-2:] in [\"st\", \"nd\", \"rd\", \"th\"]\n",
    "        ):\n",
    "            t[ind] = num2words(int(word[:-2]), to=\"ordinal\")\n",
    "\n",
    "    return \" \".join(t)\n",
    "\n",
    "\n",
    "def get_comment_thread(row: pd.Series) -> str:\n",
    "    if not row[\"text\"]:\n",
    "        return \"\"\n",
    "    if row[\"text\"][-1] not in string.punctuation:\n",
    "        row[\"text\"] += \".\"\n",
    "\n",
    "    if row[\"commentindex\"] != 0:\n",
    "        parent_df = yahoo_df[yahoo_df.commentid == row[\"parentid\"]]\n",
    "        if parent_df.shape[0] == 0:\n",
    "            return row[\"text\"]\n",
    "        else:\n",
    "            return f\"{parent_df.iloc[0].thread} {row['text']}\"\n",
    "    else:\n",
    "        return row[\"text\"]\n",
    "\n",
    "\n",
    "yahoo_df[\"text\"] = (\n",
    "    yahoo_df[\"text\"]\n",
    "    .str.replace(r'[^\\w\\s]+', '', regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .apply(lambda x: contractions.fix(x, slang=False))\n",
    "    .apply(numbers_to_words)\n",
    ")\n",
    "yahoo_df = yahoo_df.sort_values(by=[\"commentindex\"])\n",
    "yahoo_df[\"thread\"] = \"\"\n",
    "for index, row in yahoo_df.iterrows():\n",
    "    yahoo_df.at[index, \"thread\"] = get_comment_thread(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"english\"\n",
    "NUM_SENTENCES = 3\n",
    "\n",
    "tokenizer = Tokenizer(LANGUAGE)\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizer = LuhnSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "\n",
    "def summarize_thread(row: pd.Series) -> str:\n",
    "    if not row[\"thread\"]:\n",
    "        return \"\"\n",
    "    parser = PlaintextParser.from_string(row[\"thread\"], tokenizer)\n",
    "    return \"\".join([x._text for x in summarizer(parser.document, NUM_SENTENCES)])\n",
    "\n",
    "\n",
    "yahoo_df[\"summary\"] = yahoo_df.apply(summarize_thread, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_train-ids.txt\"\n",
    ") as f:\n",
    "    train_ids = [int(x) for x in f.read().splitlines()]\n",
    "\n",
    "with open(\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_dev-ids.txt\"\n",
    ") as f:\n",
    "    dev_ids = [int(x) for x in f.read().splitlines()]\n",
    "\n",
    "with open(\n",
    "    \"data/yahoo-news-annotated-comments-dataset/ydata-ynacc-v1_0_test-ids.txt\"\n",
    ") as f:\n",
    "    test_ids = [int(x) for x in f.read().splitlines()]\n",
    "\n",
    "train_df = yahoo_df[yahoo_df[\"sdid\"].isin(train_ids)]\n",
    "dev_df = yahoo_df[yahoo_df[\"sdid\"].isin(dev_ids)]\n",
    "test_df = yahoo_df[yahoo_df[\"sdid\"].isin(test_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = yahoo_df.copy(deep=True)\n",
    "temp_df[\"train\"] = temp_df[\"sdid\"].isin(train_ids)\n",
    "temp_df[\"dev\"] = temp_df[\"sdid\"].isin(dev_ids)\n",
    "temp_df[\"test\"] = temp_df[\"sdid\"].isin(test_ids)\n",
    "\n",
    "with pd.option_context(\n",
    "    \"display.max_rows\",\n",
    "    None,\n",
    "):\n",
    "    print(\n",
    "        temp_df.groupby(\n",
    "            [\n",
    "                \"sentiment\",\n",
    "                pd.cut(\n",
    "                    temp_df.commentindex,\n",
    "                    [0, 3, 6, 9, 12, 15, 18],\n",
    "                    include_lowest=True,\n",
    "                    right=False,\n",
    "                ),\n",
    "            ]\n",
    "        )[[\"train\", \"dev\", \"test\"]].sum()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_w2v_vector(row):\n",
    "    words = row['text'].split()\n",
    "    return np.sum([model[w] for w in words if w in model], axis=0) / (\n",
    "        len(words) if words else 1\n",
    "    )\n",
    "\n",
    "train_X = train_df.apply(lambda x: get_average_w2v_vector(x), axis=1, result_type=\"expand\")\n",
    "train_y = train_df['sentiment']\n",
    "\n",
    "dev_X = dev_df.apply(lambda x: get_average_w2v_vector(x), axis=1, result_type=\"expand\")\n",
    "dev_y = dev_df['sentiment']\n",
    "\n",
    "test_X = test_df.apply(lambda x: get_average_w2v_vector(x), axis=1, result_type=\"expand\")\n",
    "test_y = test_df['sentiment']\n",
    "\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(dev_X.shape, dev_y.shape)\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fce3f33274e5da354ee51a1f13b514a1195d394c6831008ceaf2ec8369c9243"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
